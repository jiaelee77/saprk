{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sparkmllib.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiaelee77/saprk/blob/master/sparkmllib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdEApEqY7s0z",
        "colab_type": "text"
      },
      "source": [
        "* Python & java8, spark 2.4.4-bin-hadoop 2.7 version (optimized)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq5Fjx9IPxIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive #\n",
        "drive.mount('/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RfAVKeywjMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx_aoSt62wym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygirFAjL2zAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYQ6wKA7318p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUo370iF4pqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "print(os.listdir('./sample_data'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l7abHdw53-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2zNJGG66OV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init(\"spark-2.4.4-bin-hadoop2.7\")# SPARK_HOME\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqIr4xtl6ioY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(os.listdir('./sample_data'))\n",
        "file_loc = './sample_data/california_housing_train.csv'\n",
        "df_spark = spark.read.csv(file_loc, inferSchema=True, header=True)\n",
        "print(type(df_spark))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb7L6kHY6vNQ",
        "colab_type": "code",
        "outputId": "d0ecf55f-b60a-4bb4-c9e2-45fe2267fb4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        }
      },
      "source": [
        "df_spark.printSchema() # print detail schema of data\n",
        "df_spark.show()# show top 20 rows\n",
        "# Do more operation on it."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- housing_median_age: double (nullable = true)\n",
            " |-- total_rooms: double (nullable = true)\n",
            " |-- total_bedrooms: double (nullable = true)\n",
            " |-- population: double (nullable = true)\n",
            " |-- households: double (nullable = true)\n",
            " |-- median_income: double (nullable = true)\n",
            " |-- median_house_value: double (nullable = true)\n",
            "\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|  -114.31|   34.19|              15.0|     5612.0|        1283.0|    1015.0|     472.0|       1.4936|           66900.0|\n",
            "|  -114.47|    34.4|              19.0|     7650.0|        1901.0|    1129.0|     463.0|         1.82|           80100.0|\n",
            "|  -114.56|   33.69|              17.0|      720.0|         174.0|     333.0|     117.0|       1.6509|           85700.0|\n",
            "|  -114.57|   33.64|              14.0|     1501.0|         337.0|     515.0|     226.0|       3.1917|           73400.0|\n",
            "|  -114.57|   33.57|              20.0|     1454.0|         326.0|     624.0|     262.0|        1.925|           65500.0|\n",
            "|  -114.58|   33.63|              29.0|     1387.0|         236.0|     671.0|     239.0|       3.3438|           74000.0|\n",
            "|  -114.58|   33.61|              25.0|     2907.0|         680.0|    1841.0|     633.0|       2.6768|           82400.0|\n",
            "|  -114.59|   34.83|              41.0|      812.0|         168.0|     375.0|     158.0|       1.7083|           48500.0|\n",
            "|  -114.59|   33.61|              34.0|     4789.0|        1175.0|    3134.0|    1056.0|       2.1782|           58400.0|\n",
            "|   -114.6|   34.83|              46.0|     1497.0|         309.0|     787.0|     271.0|       2.1908|           48100.0|\n",
            "|   -114.6|   33.62|              16.0|     3741.0|         801.0|    2434.0|     824.0|       2.6797|           86500.0|\n",
            "|   -114.6|    33.6|              21.0|     1988.0|         483.0|    1182.0|     437.0|        1.625|           62000.0|\n",
            "|  -114.61|   34.84|              48.0|     1291.0|         248.0|     580.0|     211.0|       2.1571|           48600.0|\n",
            "|  -114.61|   34.83|              31.0|     2478.0|         464.0|    1346.0|     479.0|        3.212|           70400.0|\n",
            "|  -114.63|   32.76|              15.0|     1448.0|         378.0|     949.0|     300.0|       0.8585|           45000.0|\n",
            "|  -114.65|   34.89|              17.0|     2556.0|         587.0|    1005.0|     401.0|       1.6991|           69100.0|\n",
            "|  -114.65|    33.6|              28.0|     1678.0|         322.0|     666.0|     256.0|       2.9653|           94900.0|\n",
            "|  -114.65|   32.79|              21.0|       44.0|          33.0|      64.0|      27.0|       0.8571|           25000.0|\n",
            "|  -114.66|   32.74|              17.0|     1388.0|         386.0|     775.0|     320.0|       1.2049|           44000.0|\n",
            "|  -114.67|   33.92|              17.0|       97.0|          24.0|      29.0|      15.0|       1.2656|           27500.0|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCzq9cw435qR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dataframe as df\n",
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "df.show(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdyAJI8h6Pww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7e_JbH-5Hax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy-Bij-6wITl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m1HsJkrP1FI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Run a linear regression using Apache Spark ML.\n",
        "\n",
        "In the following PySpark (Spark Python API) code, we take the following actions:\n",
        "\n",
        "  * Load a previously created linear regression (BigQuery) input table\n",
        "    into our Cloud Dataproc Spark cluster as an RDD (Resilient\n",
        "    Distributed Dataset)\n",
        "  * Transform the RDD into a Spark Dataframe\n",
        "  * Vectorize the features on which the model will be trained\n",
        "  * Compute a linear regression using Spark ML\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from datetime import datetime\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.sql.session import SparkSession\n",
        "# The imports, above, allow us to access SparkML features specific to linear\n",
        "# regression as well as the Vectors types.\n",
        "\n",
        "# Define a function that collects the features of interest\n",
        "# (mother_age, father_age, and gestation_weeks) into a vector.\n",
        "# Package the vector in a tuple containing the label (`weight_pounds`) for that\n",
        "# row.\n",
        "\n",
        "def vector_from_inputs(r):\n",
        "  return (r[\"weight_pounds\"], Vectors.dense(float(r[\"mother_age\"]),\n",
        "                                            float(r[\"father_age\"]),\n",
        "                                            float(r[\"gestation_weeks\"]),\n",
        "                                            float(r[\"weight_gain_pounds\"]),\n",
        "                                            float(r[\"apgar_5min\"])))\n",
        "\n",
        "# Use Cloud Dataprocs automatically propagated configurations to get\n",
        "# the Cloud Storage bucket and Google Cloud Platform project for this\n",
        "# cluster.\n",
        "sc = SparkContext()\n",
        "spark = SparkSession(sc)\n",
        "bucket = spark._jsc.hadoopConfiguration().get(\"fs.gs.system.bucket\")\n",
        "project = spark._jsc.hadoopConfiguration().get(\"fs.gs.project.id\")\n",
        "\n",
        "# Set an input directory for reading data from Bigquery.\n",
        "todays_date = datetime.strftime(datetime.today(), \"%Y-%m-%d-%H-%M-%S\")\n",
        "input_directory = \"gs://{}/tmp/natality-{}\".format(bucket, todays_date)\n",
        "\n",
        "# Set the configuration for importing data from BigQuery.\n",
        "# Specifically, make sure to set the project ID and bucket for Cloud Dataproc,\n",
        "# and the project ID, dataset, and table names for BigQuery.\n",
        "\n",
        "conf = {\n",
        "    # Input Parameters\n",
        "    \"mapred.bq.project.id\": project,\n",
        "    \"mapred.bq.gcs.bucket\": bucket,\n",
        "    \"mapred.bq.temp.gcs.path\": input_directory,\n",
        "    \"mapred.bq.input.project.id\": project,\n",
        "    \"mapred.bq.input.dataset.id\": \"natality_regression\",\n",
        "    \"mapred.bq.input.table.id\": \"regression_input\",\n",
        "}\n",
        "\n",
        "# Read the data from BigQuery into Spark as an RDD.\n",
        "table_data = spark.sparkContext.newAPIHadoopRDD(\n",
        "    \"com.google.cloud.hadoop.io.bigquery.JsonTextBigQueryInputFormat\",\n",
        "    \"org.apache.hadoop.io.LongWritable\",\n",
        "    \"com.google.gson.JsonObject\",\n",
        "    conf=conf)\n",
        "\n",
        "# Extract the JSON strings from the RDD.\n",
        "table_json = table_data.map(lambda x: x[1])\n",
        "\n",
        "# Load the JSON strings as a Spark Dataframe.\n",
        "natality_data = spark.read.json(table_json)\n",
        "# Create a view so that Spark SQL queries can be run against the data.\n",
        "natality_data.createOrReplaceTempView(\"natality\")\n",
        "\n",
        "# As a precaution, run a query in Spark SQL to ensure no NULL values exist.\n",
        "sql_query = \"\"\"\n",
        "SELECT *\n",
        "from natality\n",
        "where weight_pounds is not null\n",
        "and mother_age is not null\n",
        "and father_age is not null\n",
        "and gestation_weeks is not null\n",
        "\"\"\"\n",
        "clean_data = spark.sql(sql_query)\n",
        "\n",
        "# Create an input DataFrame for Spark ML using the above function.\n",
        "training_data = clean_data.rdd.map(vector_from_inputs).toDF([\"label\",\n",
        "                                                             \"features\"])\n",
        "training_data.cache()\n",
        "\n",
        "# Construct a new LinearRegression object and fit the training data.\n",
        "lr = LinearRegression(maxIter=5, regParam=0.2, solver=\"normal\")\n",
        "model = lr.fit(training_data)\n",
        "# Print the model summary.\n",
        "print (\"Coefficients:\" + str(model.coefficients))\n",
        "print (\"Intercept:\" + str(model.intercept))\n",
        "print (\"R^2:\" + str(model.summary.r2))\n",
        "model.summary.residuals.show()\n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gAgISHyvN4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUbo5hEISlB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}